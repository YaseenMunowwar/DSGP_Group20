{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Importing Libraries"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Data Analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import asarray\n",
    "from numpy import savetxt\n",
    "from numpy import loadtxt\n",
    "import pickle as pkl\n",
    "from scipy import sparse\n",
    "\n",
    "# Data Visualization\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Text Processing\n",
    "import re\n",
    "import itertools\n",
    "import string\n",
    "import collections\n",
    "from collections import Counter\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import nltk\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "# Machine Learning packages\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "import sklearn.cluster as cluster\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Model training and evaluation\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from xgboost import plot_importance\n",
    "\n",
    "#Metrics\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error, accuracy_score, balanced_accuracy_score\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, multilabel_confusion_matrix, confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Ignore noise warning\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Loading Dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data_set_personality = pd.read_csv(\"mbti_1.csv\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### printing the top tuples"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data_set_personality.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Describing the data set"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data_set_personality.describe()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# printing the number of columns and rows\n",
    "data_set_personality.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data Cleaning"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Dropping duplicates values if any are present"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data_set_personality.drop_duplicates"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# after dropping duplicates checking for the number of rows\n",
    "print(data_set_personality)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Checking for null values"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "null_values = data_set_personality.isnull().sum()\n",
    "null_values"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# checking the dataset information\n",
    "data_set_personality.dtypes"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "No null values present in the dataset and only 2 columns and 8675 rows present in the data_set_personality.\n",
    "The values are textual therefore has to be converted to numerical form to train the ML model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data Information"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data_set_personality.dtypes"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data_set_personality.info()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "types = np.unique(np.array(data_set_personality['type']))\n",
    "types"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Group by allows you to split your data into separate groups to perform computations for better analysis.\n",
    "total = data_set_personality.groupby(['type']).count()*50\n",
    "total"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data Visualization"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.figure(figsize = (12,4))\n",
    "plt.bar(np.array(total.index), height = total['posts'],)\n",
    "plt.xlabel('Personality types', size = 14)\n",
    "plt.ylabel('No. of posts available', size = 14)\n",
    "plt.title('Total posts for each personality type')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The dataset is clearly unbalanced throughout the different classes. We observe that some of the personality types has a lot more data than others, the most common Kaggle users personality is INFP (Introvert Intuition Feeling Perceiving)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Plotting this in descending order for better understanding of this visualization\n",
    "cnt_srs = data_set_personality['type'].value_counts()\n",
    "plt.figure(figsize=(12,4))\n",
    "sns.barplot(cnt_srs.index, cnt_srs.values, alpha=0.8)\n",
    "plt.xlabel('Personality types', fontsize=12)\n",
    "plt.ylabel('No. of posts availables', fontsize=12)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### SWARM PLOT"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = data_set_personality.copy()\n",
    "#this function counts the no of words in each post of a user\n",
    "def var_row(row):\n",
    "    l = []\n",
    "    for i in row.split('|||'):\n",
    "        l.append(len(i.split()))\n",
    "    return np.var(l)\n",
    "\n",
    "#this function counts the no of words per post out of the total 50 posts in the whole row\n",
    "df['words_per_comment'] = df['posts'].apply(lambda x: len(x.split())/50)\n",
    "df['variance_of_word_counts'] = df['posts'].apply(lambda x: var_row(x))\n",
    "\n",
    "plt.figure(figsize=(15,10))\n",
    "sns.swarmplot(\"type\", \"words_per_comment\", data=df)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Joint Plot"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,10))\n",
    "sns.jointplot(\"variance_of_word_counts\", \"words_per_comment\", data=df, kind=\"hex\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Joint Plots for each Personality Type"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def plot_jointplot(mbti_type, axs, titles):\n",
    "    df_1 = df[df['type'] == mbti_type]\n",
    "    sns.jointplot(\"variance_of_word_counts\", \"words_per_comment\", data=df_1, kind=\"hex\", ax = axs, title = titles)\n",
    "\n",
    "plt.figure(figsize=(24, 5))\n",
    "i = df['type'].unique()\n",
    "k = 0\n",
    "\n",
    "for m in range(1,3):\n",
    "    for n in range(1,7):\n",
    "        df_1 = df[df['type'] == i[k]]\n",
    "        sns.jointplot(\"variance_of_word_counts\", \"words_per_comment\", data=df_1, kind=\"hex\" )\n",
    "        plt.title(i[k])\n",
    "        k+=1\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "* For all the plots you can see that most of the posts have words btw 100-150 and most of no. of words per comment by a user is nearly 25 to 30 range.\n",
    "*   Exception to this case is for the plots for ISPJ and ISTJ, but this maybe due to the fact that there are significantly less no. of posts available for these personality types (further shown by the bar plots below)\n",
    "*   We can see that there is no correlation observed between variance of word count and the words per comment.\n",
    "*   But there is a weak negative correlation observed between the 2 features for few personalities. Maybe this could be due to the low no. of posts available for that type in the given Kaggle dataset.\n",
    "*   No useful inferences can be made by analyzing the individual jointplots as the total no of posts for each personlaity type is different.\n",
    "\n",
    "Hence, these features will not be useful in building our Personality prediction model."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### DISTANCE PLOT\n",
    "\n",
    "This seaborn visualization method shows the histogram distribution of data for a single column."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df[\"length_posts\"] = df[\"posts\"].apply(len)\n",
    "sns.distplot(df[\"length_posts\"]).set_title(\"Distribution of Lengths of all 50 Posts\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can see that most no of lengthly posts have between 7000-9000 words.\n",
    "The line that you see represents the kernel density estimation. It is a fundamental data smoothing problem where inferences about the population, based on a finite data sample. This kernel density estimate is a function defined as the sum of a kernel function on every data point."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Finding the most common words in all posts.\n",
    "words = list(df[\"posts\"].apply(lambda x: x.split()))\n",
    "words = [x for y in words for x in y]\n",
    "Counter(words).most_common(40)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "*   The posts majorly contain general words like : I, I'm, so, me, or, if, and, can etc. It is safe to assume that these words won't really provide any useful information to train the ML model as most of them are stop-words, stem-words, or other useless words.\n",
    "*   Hence quite a lot pre-processing is required for individual user posts for each peronality type in the given MBTI dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
